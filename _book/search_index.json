[
["index.html", "RStudio 2020 Internship Application 1 Overview", " RStudio 2020 Internship Application Riccardo Esclapon 1 Overview Video intro here https://education.rstudio.com/blog/2020/02/applications-for-2020-intern-program-are-now-open/ APPLICATIONS END ON MARCH 5TH BE SURE TO APPLY BEFORE THEN!! For video: Start off with overview of projects I am suited for showing work I did for this application specifically. Then go on to talk about ways I have applied the broad RMarkdown ecosystem and automation in my work. Then talk a bit more about myself. Talk about ideal tutorial overview and close things by mentioning cool charts/visualizations section (outline this at a high level under 2 minutes in the video at the start here) "],
["fit.html", "2 What makes me a good fit 2.1 I ❤️ .Rmd files 2.2 Automation 2.3 Fit Within the Company", " 2 What makes me a good fit Here are some of the things I believe make me a great fit for the internship: 2.1 I ❤️ .Rmd files I was completely blown away by the R Markdown file format when I first discovered it, and I definitely felt a bit cheated by the fact that none of the courses I took in college in R mentioned it at all or the tidyverse. I have spent a lot of my time learning R Markdown and digging through books and amazing resources made available by RStudio, so here are some of my favorite formats that I would love to make more content around and teach people about: 2.1.1 Learnr I have been using learnr for about a year and a half, and recently I started to offer tutorials on my website using learnr where every time the tutorial is opened, users learn to program in R using data from the cryptocurrency markets that is never outdated by more than 1 hour: (this takes about 45 seconds to load, give it more time if it’s showing up blank) I would recommend looking at the Visualization section to visually see that the data is never outdated by more than 1 hour. I post these on my website: I’m loving the integrated tutorials tab within RStudio in the 1.3 preview and I am working towards including these with my PredictCrypto package, which I talk more about and use in the next section of this document. 2.1.2 Bookdown I was very close to paying for a monthly subscription on gitbook.com because I thought it was such an amazing format to provide documentation through, so I was particularly impressed by and grateful for the bookdown (Xie 2020) package, and these days it’s my go to for organizing most things I work on, so why not my application? This document is obviously an example of a bookdown document in itself, but here’s another guide I put together using bookdown: This guide refreshes daily in order to show a preview of the latest data within the document and you can look at the GitHub Actions daily runs here How does URL itself look here (DELETE THIS:) I also found that documentation done in bookdown can work really great when working within a large company as well, and I put together some very thorough documentation for a project using bookdown that was very well received (but I can’t show here). In my particular case it worked really well because I could send the link to the html index of the bookdown document and when opened it would behave like a website hosted on the shared folders within the secure network which ended up being particularly simple and effective. 2.1.3 Presentations I am a big fan of ioslides and revealjs in particular as R Markdown outputs. I find the revealjs output to be incredibly cool with the rotating cube animation, and the ability to not only move forward but move downward adds a surprisingly useful tool to break down topics; ioslides is just really clean, well made and easy to use and looks great with widescreen enabled. I aspire to be an expert in Xaringan one day but am not currently. Making presentations in R Markdown is what really got me working with .Rmd files, because I started working towards a very specific project using an idea I haven’t really seen elsewhere of creating presentations that give the user options and as they make their way through the slides, those options affect not only what they see in the slides that come afterwards, but also the options they are given. For example, the user could choose to do an analysis for a particular asset, then choose the main category of the analysis to perform, then the sub-category of the analysis and so on, until by the end of the presentation the user has performed an analysis that was completely unique and tailored to their preferences and interests. See the gif below for an example of what this looks like: 2.1.4 Blogdown Blogdown(Xie 2019) and bookdown work very similarly, so most of what I mentioned in the bookdown section applies here. Because my website predictcrypto.com only shows the latest data based on the current date, I leverage blogdown to create weekly snapshots of the visualizations over the last 7 day period: https://predictcryptoblog.com/. Because all these systems work so well with automation, as I keep adding new interesting content to my website I can also add archives of that content using blogdown. 2.1.5 Pagedown Pagedown(Xie, Lesur, and Thorne 2020) is yet another awesome way to create html outputs and I used Nick Strayer’s repository https://github.com/nstrayer/cv to build my cv and resume using his template: 2.1.6 Flexdashboard Flexdashboards(Iannone, Allaire, and Borges 2018) were my first introduction to shiny apps and I was completely blown away by that framework and have used it for several projects and is one of my absolute favorite tools. To get some practice, I converted some of the content found in Tidy Text Mining by Julia Silge and David Robinson and made it into a flexdashboard. I made no changes to the code found within the book, this was simply an experiment to learn more about flexdashboards and semantic analysis: I made the code available through RStudio Cloud here as well: https://rstudio.cloud/spaces/9369/join?access_code=pkfhGuOMRhleNIHSHH6YOQPEWstEdg0e7Pi6Ue3q 2.2 Automation Automation is at the center of everything I do and my one true passion. One of my big goals for RStudio::conf 2020 was to learn more about automating things through GitHub using CI since I always had a hard time figuring that out, and the things I learned about especially relating to GitHub actions and using Netlify were above my expectations in terms of the ease of use, capabilities and free tier offerings, and I am super excited to share how crazy simple automating a very complex process can be through RStudio, GitHub Actions and Netlify and I’m not sure why people make things so . The bookdown example from earlier https://predictcryptodb-quickstart.com/ for example uses those tools to refresh the guide daily in order to show the latest data in the useful tables section It’s pretty mindblowing that these frameworks allow a user to create an interactive book with complex javascript, HTML, CSS, TeX, etc… from scratch, deploy it to an https secured website and create an automated process around it, all in less than 10 minutes with minimal code involved. What’s even more mindblowing, is that the same methodologies can be applied to make other interfaces, like making a blogdown website, and I can’t speak highly enough of all the work Yihui blessed us all with. 2.3 Fit Within the Company I really wanted to go to RStudio::conf 2019 but was not able to make it out and after all the videos got posted I watched most of them and immediately knew I had to come to RStudio::conf 2020 and it was a truly incredible experience. JJ’s talk and BCorp announcement really resonated with me and there is no other company who’s mission I agree with more and I would always do my very best in carrying forward those values. I fundamentally believe the most straightforward way to success is to help other people succeed, and I love the values that RStudio holds dear as a company and there is really no other company that I want to work for more than RStudio. References "],
["ideal-projects.html", "3 Projects Well Suited For 3.1 Create resources for people working with spreadsheets in R 3.2 Build interactive learnr tutorials for tidymodels 3.3 Build interactive learnr tutorials for Python using reticulate", " 3 Projects Well Suited For 3.1 Create resources for people working with spreadsheets in R What better way to show I am suited for a project than to give a hands-on example? See the code below for a use-case using googlesheets4(Bryan 2020). First I will go ahead and import every package in the tidyverse(Wickham 2019): library(tidyverse) ## -- Attaching packages -------------------------------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.2.1 v purrr 0.3.3 ## v tibble 2.1.3 v dplyr 0.8.4 ## v tidyr 1.0.0 v stringr 1.4.0 ## v readr 1.3.1 v forcats 0.4.0 ## -- Conflicts ----------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Next let’s import the googlesheets4 and read a spreadsheet I made for this internship application, specifying the sheet called coinmetrics_btc_eth inside the function read_sheet(): spreadsheet_url &lt;- &quot;https://docs.google.com/spreadsheets/d/1_zRBFrB1au7qhxuDDfDuh_bPLGd6RLrwOL5oQ3sBBX4/edit?usp=sharing&quot; library(googlesheets4) googlesheets_data &lt;- read_sheet(spreadsheet_url, sheet = &#39;coinmetrics_btc_eth&#39;) %&gt;% as.data.frame() ## Using an auto-discovered, cached token. ## To suppress this message, modify your code or options to clearly consent to the use of a cached token. ## See gargle&#39;s &quot;Non-interactive auth&quot; vignette for more details: ## https://gargle.r-lib.org/articles/non-interactive-auth.html ## The googlesheets4 package is using a cached token for ries9112@colorado.edu. Let’s take a peek at the datatable() using DT (Xie, Cheng, and Tan 2019) library(DT) datatable(googlesheets_data, style = &quot;default&quot;, options = list(scrollX = TRUE, pageLength=5,dom=&#39;t&#39;), rownames = F) ## Warning in instance$preRenderHook(instance): It seems your data is too big ## for client-side DataTables. You may consider server-side processing: https:// ## rstudio.github.io/DT/server.html This data is sourced from the website coinmetrics.io Coinmetrics also provides a data dictionary to go along with the data: 3.1.1 Predictive Model - Data Prep Using the data from coinmetrics, I will create a predictive model to forecast the percentage change in price over time in order to show that I would be a good fit for the first two projects listed. First, I will import a package that I am making that is still in development PredictCrypto: library(PredictCrypto) (this is an in-development tool that I will use for a research paper I am working on) I attended the two day building tidy tools workshop working with Charlotte and Hadley at RStudio::conf 2020 and I am comfortable writing packages in R as well as using testthat and showing code coverage for a repository. Here is the GitHub Pages environment associated with the repository: https://ries9112.github.io/PredictCrypto/ I am going to convert the column names from CamelCase to snake_case using the janitor(Firke 2020) package because the functions in my package use snake_case and I want to avoid mixing the two: Before: ## [1] &quot;Date&quot; &quot;Symbol&quot; &quot;AdrActCnt&quot; &quot;BlkCnt&quot; ## [5] &quot;BlkSizeByte&quot; &quot;BlkSizeMeanByte&quot; &quot;CapMVRVCur&quot; &quot;CapMrktCurUSD&quot; ## [9] &quot;CapRealUSD&quot; &quot;DiffMean&quot; &quot;FeeMeanNtv&quot; &quot;FeeMeanUSD&quot; ## [13] &quot;FeeMedNtv&quot; &quot;FeeMedUSD&quot; &quot;FeeTotNtv&quot; &quot;FeeTotUSD&quot; ## [17] &quot;HashRate&quot; &quot;IssContNtv&quot; &quot;IssContPctAnn&quot; &quot;IssContUSD&quot; ## [21] &quot;IssTotNtv&quot; &quot;IssTotUSD&quot; &quot;NVTAdj&quot; &quot;NVTAdj90&quot; ## [25] &quot;PriceBTC&quot; &quot;PriceUSD&quot; &quot;ROI1yr&quot; &quot;ROI30d&quot; ## [29] &quot;SplyCur&quot; &quot;TxCnt&quot; &quot;TxTfrCnt&quot; &quot;TxTfrValAdjNtv&quot; ## [33] &quot;TxTfrValAdjUSD&quot; &quot;TxTfrValMeanNtv&quot; &quot;TxTfrValMeanUSD&quot; &quot;TxTfrValMedNtv&quot; ## [37] &quot;TxTfrValMedUSD&quot; &quot;TxTfrValNtv&quot; &quot;TxTfrValUSD&quot; &quot;VtyDayRet180d&quot; ## [41] &quot;DateTimeUTC&quot; library(janitor) googlesheets_data &lt;- clean_names(googlesheets_data) After: ## [1] &quot;date&quot; &quot;symbol&quot; &quot;adr_act_cnt&quot; ## [4] &quot;blk_cnt&quot; &quot;blk_size_byte&quot; &quot;blk_size_mean_byte&quot; ## [7] &quot;cap_mvrv_cur&quot; &quot;cap_mrkt_cur_usd&quot; &quot;cap_real_usd&quot; ## [10] &quot;diff_mean&quot; &quot;fee_mean_ntv&quot; &quot;fee_mean_usd&quot; ## [13] &quot;fee_med_ntv&quot; &quot;fee_med_usd&quot; &quot;fee_tot_ntv&quot; ## [16] &quot;fee_tot_usd&quot; &quot;hash_rate&quot; &quot;iss_cont_ntv&quot; ## [19] &quot;iss_cont_pct_ann&quot; &quot;iss_cont_usd&quot; &quot;iss_tot_ntv&quot; ## [22] &quot;iss_tot_usd&quot; &quot;nvt_adj&quot; &quot;nvt_adj90&quot; ## [25] &quot;price_btc&quot; &quot;price_usd&quot; &quot;roi1yr&quot; ## [28] &quot;roi30d&quot; &quot;sply_cur&quot; &quot;tx_cnt&quot; ## [31] &quot;tx_tfr_cnt&quot; &quot;tx_tfr_val_adj_ntv&quot; &quot;tx_tfr_val_adj_usd&quot; ## [34] &quot;tx_tfr_val_mean_ntv&quot; &quot;tx_tfr_val_mean_usd&quot; &quot;tx_tfr_val_med_ntv&quot; ## [37] &quot;tx_tfr_val_med_usd&quot; &quot;tx_tfr_val_ntv&quot; &quot;tx_tfr_val_usd&quot; ## [40] &quot;vty_day_ret180d&quot; &quot;date_time_utc&quot; Now that I imported the PredictCrypto package and the data is in snake_case, I can use the function calculate_percent_change() to create the target variable to predict. Before I can do that however, I need one more adjustment to the date/time fields, so let’s do that using the anytime(Eddelbuettel 2020) package: library(anytime) googlesheets_data$date &lt;- anytime(googlesheets_data$date) googlesheets_data$date_time_utc &lt;- anytime(googlesheets_data$date_time_utc) Now I can use the function calculate_percent_change() to calculate the % change of the price of each cryptocurrency and add a new column target_percent_change to each row, which will represent the percentage change in price for the 7 day period that came after that data point was collected: exercise_data &lt;- PredictCrypto::calculate_percent_change(googlesheets_data, 7, &#39;days&#39;) Let’s take a peek at the new field: tail(exercise_data$target_percent_change, 10) ## [1] 2.371123 7.142857 20.891949 23.846718 6.707045 19.548486 ## [7] -25.610849 -21.561229 -30.693069 -41.121855 I could easily change this to a 14 day period: calculate_percent_change(googlesheets_data, 14, &#39;days&#39;) %&gt;% tail(10) %&gt;% select(target_percent_change) ## target_percent_change ## 5104 18.281533 ## 5105 25.714286 ## 5106 20.496231 ## 5107 8.184818 ## 5108 1.779083 ## 5109 4.327005 ## 5110 -28.003738 ## 5111 -19.701349 ## 5112 -25.742574 ## 5113 -28.821062 Or a 24 hour period: calculate_percent_change(googlesheets_data, 24, &#39;hours&#39;) %&gt;% tail(10) %&gt;% select(target_percent_change) ## target_percent_change ## 5130 0.492989 ## 5131 4.682143 ## 5132 10.801132 ## 5133 -7.332233 ## 5134 -9.989603 ## 5135 3.630922 ## 5136 -26.167717 ## 5137 5.963659 ## 5138 -7.504950 ## 5139 -5.871389 Let’s also scale the variables ADD FEATURE SCALING HERE 3.2 Build interactive learnr tutorials for tidymodels https://education.rstudio.com/blog/2020/02/conf20-intro-ml/ https://conf20-intro-ml.netlify.com/materials/01-predicting/ Disclaimer:Most of the code to follow was built using the content made available by Allison Hill from the RStudio::conf2020 intro to machine learning workshop and was not code I was familiar with before writing it for this internship application. 3.2.1 Create the parsnip (Kuhn and Vaughan 2020) model library(parsnip) lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) List of models to refer to: https://tidymodels.github.io/parsnip/articles/articles/Models.html Random Forest: random_forest_model &lt;- rand_forest(trees = 100, mode = &quot;regression&quot;) %&gt;% set_engine(&quot;randomForest&quot;) XGBoost: xgboost_model &lt;- xgboost_parsnip &lt;- boost_tree() %&gt;% set_engine(&quot;xgboost&quot;) %&gt;% set_mode(&quot;regression&quot;) Remove fields not used for models (NOTE: REMOVE price_usd_x_daysLater AND date_time_x_days_Later FROM BEING GENERATED INSIDE THE FUNCTION) exercise_data &lt;- select(exercise_data, -date_time_utc, -date_time, -pkDummy, -pkey, -price_usd_x_daysLater, -date_time_x_daysLater) Create train/test split using rsample(Kuhn, Chow, and Wickham 2019): (should I do 10-fold cross validation?) library(rsample) set.seed(250) crypto_data &lt;- initial_split(exercise_data, prop = 0.8) crypto_train &lt;- training(crypto_data) crypto_test &lt;- testing(crypto_data) 3.2.2 Train/fit the model: library(modelr) lm_fitted &lt;- lm_model %&gt;% fit(price_usd ~ ., data=crypto_train) Random Forest: random_forest_fitted &lt;- random_forest_model %&gt;% fit(target_percent_change ~ ., data = exercise_data) XGBoost: xgboost_fitted &lt;- xgboost_model %&gt;% fit(price_usd ~ ., data=crypto_train) Use the trained model to make predictions on test data: library(tidymodels) ## -- Attaching packages ------------------------------------------------------- tidymodels 0.1.0 -- ## v broom 0.5.4 v tune 0.0.1 ## v dials 0.0.4 v workflows 0.1.0 ## v infer 0.5.1 v yardstick 0.0.5 ## v recipes 0.1.9 ## -- Conflicts ---------------------------------------------------------- tidymodels_conflicts() -- ## x broom::bootstrap() masks modelr::bootstrap() ## x scales::discard() masks purrr::discard() ## x dplyr::filter() masks stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x yardstick::mae() masks modelr::mae() ## x yardstick::mape() masks modelr::mape() ## x dials::margin() masks ggplot2::margin() ## x yardstick::rmse() masks modelr::rmse() ## x yardstick::spec() masks readr::spec() ## x recipes::step() masks stats::step() ## x recipes::yj_trans() masks scales::yj_trans() lm_predictions &lt;- lm_fitted %&gt;% predict(crypto_test) xgboost_predictions &lt;- xgboost_fitted %&gt;% predict(crypto_test) Join the full dataset back to the predictions: lm_predictions &lt;- lm_predictions %&gt;% bind_cols(crypto_test) xgboost_predictions &lt;- xgboost_predictions %&gt;% bind_cols(crypto_test) Get metrics: lm_predictions %&gt;% metrics(truth = target_percent_change, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 3580. ## 2 rsq standard 0.000277 ## 3 mae standard 1749. xgboost_predictions %&gt;% metrics(truth = target_percent_change, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 3316. ## 2 rsq standard 0.000975 ## 3 mae standard 1496. 3.2.3 Now make one model for each cryptocurrency. Code adapted from: https://r4ds.had.co.nz/many-models.html First I group the data crypto_data_grouped &lt;- exercise_data %&gt;% group_by(symbol) %&gt;% nest() crypto_data_grouped ## # A tibble: 2 x 2 ## # Groups: symbol [2] ## symbol data ## &lt;chr&gt; &lt;list&gt; ## 1 BTC &lt;tibble [3,502 x 40]&gt; ## 2 ETH &lt;tibble [1,625 x 40]&gt; Make a helper function with the model: grouped_linear_model &lt;- function(df) { lm(target_percent_change ~ ., data = df) } Now we can use purrr(Henry and Wickham 2019) to apply the model to each element of the grouped dataframe: grouped_models &lt;- map(crypto_data_grouped$data, grouped_linear_model) The models can be added into the dataframe as nested lists. We can also add the corresponding residuals: crypto_data_grouped &lt;- crypto_data_grouped %&gt;% mutate(model=map(data,grouped_linear_model)) %&gt;% mutate(resids = map2(data, model, add_residuals)) Let’s look at the object again: crypto_data_grouped ## # A tibble: 2 x 4 ## # Groups: symbol [2] ## symbol data model resids ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 BTC &lt;tibble [3,502 x 40]&gt; &lt;lm&gt; &lt;tibble [3,502 x 41]&gt; ## 2 ETH &lt;tibble [1,625 x 40]&gt; &lt;lm&gt; &lt;tibble [1,625 x 41]&gt; Let’s unnest the residuals to take a closer look: resids &lt;- unnest(crypto_data_grouped, resids) resids %&gt;% ggplot(aes(date, resid)) + geom_line(aes(group = symbol), alpha = 1 / 3) + geom_smooth(se = FALSE) + ylim(c(-20,20)) + facet_wrap(~symbol) 3.2.4 Add error metrics using broom (Robinson and Hayes 2020): crypto_models_metrics &lt;- crypto_data_grouped %&gt;% mutate(metrics=map(model,broom::glance)) %&gt;% unnest(metrics) Sort by the best scores: crypto_models_metrics %&gt;% arrange(-r.squared) ## # A tibble: 2 x 15 ## # Groups: symbol [2] ## symbol data model resids r.squared adj.r.squared sigma statistic p.value ## &lt;chr&gt; &lt;lis&gt; &lt;lis&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BTC &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.158 0.148 12.4 16.1 4.47e-90 ## 2 ETH &lt;tib~ &lt;lm&gt; &lt;tibb~ 0.0341 0.00552 3203. 1.19 1.99e- 1 ## # ... with 6 more variables: df &lt;int&gt;, logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, ## # deviance &lt;dbl&gt;, df.residual &lt;int&gt; 3.2.5 How much better do the models get if we add more variables? Add MA, EMA, etc… 3.2.6 Next Steps: I won’t go further than this here, but as my next steps, here is what I would do: Use parsnip + purrr to iterate through lots of predictive models How much better do the models get with hyperparameter tuning? Visualize the best model before and after parameter tuning and then do the same with the worst performing model 3.3 Build interactive learnr tutorials for Python using reticulate I think I could be a great fit for the third project listed related to creating learnr tutorials for Python using reticulate. I have a fair amount of experience in Python, but it’s never really clicked very much for as much as R in the past, and I am looking to step-up my Python skills. My Master’s in Data Science will work with Python a lot, and people immediately ask if I make tutorials in Python when I show them the R tutorials I have made, so this would be a great one for me to work on. I am also constantly told that Python is better than R for the incorrect reasons, and being more of an expert in Python would certainly help me debunk that myth when someone makes that argument. I am very familiar with the reticulate package and I have used it in the past in an RMarkdown file to make automated cryptocurrency trades through a Python package shrimpy-python, which worked really well: https://github.com/shrimpy-dev/shrimpy-python Since I have already demonstrated my familiarity with learnr tutorials in the previous section, I will keep going with the code from the tidymodels project example I just finished and use Python for … ADD GOAL HERE … DEV: Replace this with the Python one: Could make a very simple xgboost model maybe? Could also show using Shrimpy API to pull latest data, manipulate in pandas and visualize Mention experience/courses taken in Python and how it’s never clicked with me very much but how I am taking a basic Python course in my Master’s in Data Science and I am looking to take it as an opportunity to create a lot of content using reticulate. library(reticulate) References "],
["about-me.html", "4 About Me", " 4 About Me "],
["ideal-tutorial.html", "5 Ideal Tutorial", " 5 Ideal Tutorial "],
["cool-charts.html", "6 Cool Charts 6.1 Disable while working on bookdown, takes too long to render!", " 6 Cool Charts 6.1 Disable while working on bookdown, takes too long to render! Here are some examples of charts, which refresh daily using GitHub actions and Netlify for automation. ## [1] TRUE "],
["references.html", "References", " References "]
]
